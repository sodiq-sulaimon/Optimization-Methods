{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2d22d",
   "metadata": {},
   "source": [
    "# Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a631ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03795f0a",
   "metadata": {},
   "source": [
    "#### 1. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416dfb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters with gradient descent\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] =  parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] =  parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dba0e",
   "metadata": {},
   "source": [
    "#### 2. Mini-Batch Gradient Descent\n",
    "Random mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113a83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77138a",
   "metadata": {},
   "source": [
    "#### 3. Momentum\n",
    "Initialize velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ce330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str (l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        v[\"db\" + str (l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01733c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l)] = beta * v[\"dW\" + str(l)] + ((1 - beta) * grads[\"dW\" + str(l)])\n",
    "        v[\"db\" + str(l)] = beta * v[\"db\" + str(l)] + ((1 - beta) * grads[\"db\" + str(l)])\n",
    "        \n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l)] -= (learning_rate * v[\"dW\" + str(l)])\n",
    "        parameters[\"b\" + str(l)] -= (learning_rate * v[\"db\" + str(l)])\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9f2f9",
   "metadata": {},
   "source": [
    "**What to remember:**\n",
    "\n",
    "Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.\n",
    "You have to tune a momentum hyperparameter  ð›½  and a learning rate  ð›¼ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb8138f",
   "metadata": {},
   "source": [
    "#### 4. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49763683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        v[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "        s[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        s[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e16bf",
   "metadata": {},
   "source": [
    "#### 4.1 Update parameters with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aba44f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "\n",
    "        v[\"dW\" + str(l)] = (beta1 * v[\"dW\" + str(l)]) + ((1 - beta1) * grads[\"dW\" + str(l)])\n",
    "        v[\"db\" + str(l)] = (beta1 * v[\"db\" + str(l)]) + ((1 - beta1) * grads[\"db\" + str(l)])\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - (beta1 ** 2))\n",
    "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - (beta1 ** 2))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + ((1 - beta2) * grads[\"dW\" + str(l)] ** 2)\n",
    "        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + ((1 - beta2) * grads[\"db\" + str(l)] ** 2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - beta2 ** t)\n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - beta2 ** t)\n",
    "        \n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * (v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon))\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * (v_corrected[\"db\" + str(l)] / (np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon))\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917753ed",
   "metadata": {},
   "source": [
    "#### 5. Learning Rate Decay and Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c491f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True, decay=None, decay_rate=1):\n",
    "    \n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate   # the original learning rate\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af7b96",
   "metadata": {},
   "source": [
    "#### 5.1 Decay on every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ca50d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "    learning_rate = 1 / (1 + decay_rate * epoch_num) * learning_rate0\n",
    "    \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34166eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original learning rate:  0.5\n",
      "Updated learning rate:  0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "print(\"Original learning rate: \", learning_rate)\n",
    "epoch_num = 2\n",
    "decay_rate = 1\n",
    "learning_rate_2 = update_lr(learning_rate, epoch_num, decay_rate)\n",
    "\n",
    "print(\"Updated learning rate: \", learning_rate_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4220795",
   "metadata": {},
   "source": [
    "#### 5.2. Fixed interval scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b2148e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n",
    "    learning_rate = 1 / (1 + decay_rate * math.floor(epoch_num/time_interval)) * learning_rate0\n",
    "    \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b44aa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original learning rate:  0.5\n",
      "Updated learning rate after 10 epochs:  0.5\n",
      "Updated learning rate after 100 epochs:  0.3846153846153846\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "print(\"Original learning rate: \", learning_rate)\n",
    "\n",
    "epoch_num_1 = 10\n",
    "epoch_num_2 = 100\n",
    "decay_rate = 0.3\n",
    "time_interval = 100\n",
    "learning_rate_1 = schedule_lr_decay(learning_rate, epoch_num_1, decay_rate, time_interval)\n",
    "learning_rate_2 = schedule_lr_decay(learning_rate, epoch_num_2, decay_rate, time_interval)\n",
    "print(\"Updated learning rate after {} epochs: \".format(epoch_num_1), learning_rate_1)\n",
    "print(\"Updated learning rate after {} epochs: \".format(epoch_num_2), learning_rate_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50a944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
